// lib/llmFactory.js
// LangChain ç»Ÿä¸€æ¨¡å‹å·¥å‚ - å°†æ‰€æœ‰AIæ¨¡å‹æä¾›å•†ç»Ÿä¸€åˆ°æ ‡å‡†æ¥å£

import { ChatOllama } from "@langchain/community/chat_models/ollama";
import { ChatOpenAI } from "@langchain/openai";
import { SimpleChatModel } from "@langchain/core/language_models/chat_models";
import axios from 'axios';
import { getConfig } from "../config/index.js";

// --- ä¸ºé OpenAI å…¼å®¹çš„æ¨¡å‹åˆ›å»ºè‡ªå®šä¹‰ LangChain ç±» ---

/**
 * è‡ªå®šä¹‰DashScope (é€šä¹‰åƒé—®) èŠå¤©æ¨¡å‹
 * å®ç°LangChainæ¥å£ä»¥æ”¯æŒé˜¿é‡Œäº‘é€šä¹‰åƒé—®API
 */
class CustomDashScopeChat extends SimpleChatModel {
  constructor(fields) {
    super(fields);
    this.modelId = fields.modelId;
    this.apiKey = fields.apiKey;
    this.baseUrl = fields.baseUrl || "https://dashscope.aliyuncs.com/api/v1";
    this.temperature = fields.temperature ?? 0.7;
  }

  _llmType() {
    return "custom_dashscope_chat";
  }

  async _call(messages, options) {
    try {
      // è½¬æ¢æ¶ˆæ¯æ ¼å¼åˆ°DashScopeæ ¼å¼
      const history = messages.map(msg => ({
        role: msg.role === 'ai' ? 'assistant' : msg.role,
        content: msg.content
      }));

      const apiUrl = `${this.baseUrl}/services/aigc/text-generation/generation`;

      const requestBody = {
        model: this.modelId,
        input: { messages: history },
        parameters: {
          temperature: this.temperature,
          max_tokens: options?.max_tokens || 2000
        }
      };

      console.log(`ğŸŒ è°ƒç”¨DashScope API: ${this.modelId}`);

      const response = await axios.post(apiUrl, requestBody, {
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        timeout: options?.timeout || 30000
      });

      if (response.data?.output?.text) {
        return response.data.output.text;
      } else if (response.data?.output?.choices?.[0]?.message?.content) {
        return response.data.output.choices[0].message.content;
      } else {
        throw new Error(`DashScope APIå“åº”æ ¼å¼å¼‚å¸¸: ${JSON.stringify(response.data)}`);
      }
    } catch (error) {
      console.error(`âŒ DashScope APIè°ƒç”¨å¤±è´¥:`, error.message);
      throw new Error(`DashScopeæ¨¡å‹è°ƒç”¨å¤±è´¥: ${error.response?.data?.message || error.message}`);
    }
  }
}

/**
 * æ ¹æ®æ¨¡å‹é…ç½®åŠ¨æ€åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ª LangChain ChatModel å®ä¾‹
 * è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒæŠ½è±¡ - æ‰€æœ‰æ¨¡å‹éƒ½é€šè¿‡è¿™ä¸ªå·¥å‚ç»Ÿä¸€åˆ›å»º
 *
 * @param {object} modelConfig - æ¥è‡ª config/models.js çš„æ¨¡å‹å¯¹è±¡
 * @param {string} [apiKey] - ç”¨æˆ·æä¾›çš„ API Key (å¯¹éœ€è¦å¯†é’¥çš„æ¨¡å‹)
 * @returns {import("@langchain/core/language_models/chat_models").BaseChatModel}
 * @throws {Error} å½“æ¨¡å‹é…ç½®æ— æ•ˆæˆ–åˆ›å»ºå¤±è´¥æ—¶
 */
export function createChatModel(modelConfig, apiKey) {
  if (!modelConfig) {
    throw new Error('æ¨¡å‹é…ç½®ä¸èƒ½ä¸ºç©º');
  }

  const { provider, modelId, is_openai_compatible, base_url, requires_api_key } = modelConfig;

  // éªŒè¯APIå¯†é’¥è¦æ±‚
  if (requires_api_key && !apiKey) {
    throw new Error(`æ¨¡å‹ ${modelConfig.name} éœ€è¦APIå¯†é’¥ï¼Œä½†æœªæä¾›`);
  }

  console.log(`ğŸ­ LLMå·¥å‚åˆ›å»ºæ¨¡å‹: ${modelConfig.name} (${provider})`);

  try {
    // ä¼˜å…ˆå¤„ç†æ‰€æœ‰ OpenAI å…¼å®¹çš„æ¨¡å‹ - è¿™æ˜¯æœ€ç®€å•çš„æƒ…å†µ
    if (is_openai_compatible) {
      console.log(`ğŸ”— ä½¿ç”¨OpenAIå…¼å®¹æ¥å£: ${base_url}`);
      return new ChatOpenAI({
        apiKey: apiKey,
        modelName: modelId,
        configuration: {
          baseURL: base_url
        },
        temperature: 0.7,
        maxTokens: 2000,
      });
    }

    // å¤„ç†éå…¼å®¹çš„ç‰¹æ®Šæ¨¡å‹
    switch (provider) {
      case 'Ollama': {
        const ollamaConfig = getConfig('ai.ollama');
        const baseUrl = ollamaConfig?.baseUrl || 'http://localhost:11434';
        console.log(`ğŸ–¥ï¸ ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹: ${baseUrl}`);
        return new ChatOllama({
          baseUrl: baseUrl,
          model: modelId,
          temperature: 0.7,
        });
      }

      case 'ModelScope': {
        console.log(`âš¡ ä½¿ç”¨è‡ªå®šä¹‰DashScopeæ¨¡å‹: ${modelId}`);
        return new CustomDashScopeChat({
          apiKey: apiKey,
          modelId: modelId,
          baseUrl: base_url,
          temperature: 0.7,
        });
      }

      default:
        throw new Error(`ä¸æ”¯æŒçš„ LLM provider: ${provider}ã€‚è¯·æ£€æŸ¥æ¨¡å‹é…ç½®æˆ–å®ç°å¯¹åº”çš„æ¨¡å‹ç±»ã€‚`);
    }
  } catch (error) {
    console.error(`âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥:`, error.message);
    throw new Error(`æ— æ³•åˆ›å»ºæ¨¡å‹ ${modelConfig.name}: ${error.message}`);
  }
}

/**
 * éªŒè¯æ¨¡å‹æ˜¯å¦å¯ç”¨
 * å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¶ˆæ¯æ¥æ£€æŸ¥æ¨¡å‹è¿æ¥æ€§
 *
 * @param {object} modelConfig - æ¨¡å‹é…ç½®
 * @param {string} [apiKey] - APIå¯†é’¥
 * @returns {Promise<boolean>} æ¨¡å‹æ˜¯å¦å¯ç”¨
 */
export async function validateModel(modelConfig, apiKey) {
  try {
    const model = createChatModel(modelConfig, apiKey);

    // å¯¼å…¥æ¶ˆæ¯ç±»å‹
    const { HumanMessage } = await import("@langchain/core/messages");

    // å‘é€ç®€å•æµ‹è¯•æ¶ˆæ¯
    const testMessage = new HumanMessage("æµ‹è¯•è¿æ¥");
    const response = await model.invoke([testMessage], { timeout: 10000 });

    console.log(`âœ… æ¨¡å‹ ${modelConfig.name} éªŒè¯æˆåŠŸ`);
    return true;
  } catch (error) {
    console.error(`âŒ æ¨¡å‹ ${modelConfig.name} éªŒè¯å¤±è´¥:`, error.message);
    return false;
  }
}

/**
 * è·å–æ”¯æŒçš„æ¨¡å‹æä¾›å•†åˆ—è¡¨
 * @returns {string[]} æä¾›å•†åç§°æ•°ç»„
 */
export function getSupportedProviders() {
  return ['Ollama', 'OpenAI', 'æ™ºè°±', 'ModelScope'];
}

/**
 * æ£€æŸ¥æ˜¯å¦ä¸ºOpenAIå…¼å®¹æ¨¡å‹
 * @param {object} modelConfig - æ¨¡å‹é…ç½®
 * @returns {boolean} æ˜¯å¦ä¸ºOpenAIå…¼å®¹
 */
export function isOpenAICompatible(modelConfig) {
  return Boolean(modelConfig?.is_openai_compatible);
}