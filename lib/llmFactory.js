// lib/llmFactory.js
// LangChain ç»Ÿä¸€æ¨¡å‹å·¥å‚ - å°†æ‰€æœ‰AIæ¨¡å‹æä¾›å•†ç»Ÿä¸€åˆ°æ ‡å‡†æ¥å£

import { ChatOllama } from "@langchain/community/chat_models/ollama";
import { ChatOpenAI } from "@langchain/openai";
import { SimpleChatModel } from "@langchain/core/language_models/chat_models";
import axios from "axios";
import { getConfig } from "../config/index.js";
// æ–°å¢: åœ¨æ–‡ä»¶é¡¶éƒ¨è¿›è¡Œé™æ€å¯¼å…¥ï¼Œè§£å†³ await é”™è¯¯
import { getModelById } from "../config/models.js";

// --- ä¸ºé OpenAI å…¼å®¹çš„æ¨¡å‹åˆ›å»ºè‡ªå®šä¹‰ LangChain ç±» ---

/**
 * è‡ªå®šä¹‰DashScope (é€šä¹‰åƒé—®) èŠå¤©æ¨¡å‹
 * (è¿™éƒ¨åˆ†ä»£ç ä¿æŒä¸å˜ï¼Œè®¾è®¡è‰¯å¥½)
 */
class CustomDashScopeChat extends SimpleChatModel {
  constructor(fields) {
    super(fields);
    this.modelId = fields.modelId;
    this.apiKey = fields.apiKey;
    this.baseUrl = fields.baseUrl || "https://dashscope.aliyuncs.com/api/v1";
    this.temperature = fields.temperature ?? 0.7;
  }

  _llmType() {
    return "custom_dashscope_chat";
  }

  async _call(messages, options) {
    try {
      const history = messages.map((msg) => ({
        role: msg.role === "ai" ? "assistant" : msg.role,
        content: msg.content,
      }));
      const apiUrl = `${this.baseUrl}/services/aigc/text-generation/generation`;
      const requestBody = {
        model: this.modelId,
        input: { messages: history },
        parameters: {
          temperature: this.temperature,
          max_tokens: options?.max_tokens || 2000,
        },
      };
      console.log(`ğŸŒ è°ƒç”¨DashScope API: ${this.modelId}`);
      const response = await axios.post(apiUrl, requestBody, {
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          "Content-Type": "application/json",
        },
        timeout: options?.timeout || 30000,
      });
      if (response.data?.output?.text) {
        return response.data.output.text;
      } else if (response.data?.output?.choices?.[0]?.message?.content) {
        return response.data.output.choices[0].message.content;
      } else {
        throw new Error(
          `DashScope APIå“åº”æ ¼å¼å¼‚å¸¸: ${JSON.stringify(response.data)}`
        );
      }
    } catch (error) {
      console.error(`âŒ DashScope APIè°ƒç”¨å¤±è´¥:`, error.message);
      throw new Error(
        `DashScopeæ¨¡å‹è°ƒç”¨å¤±è´¥: ${
          error.response?.data?.message || error.message
        }`
      );
    }
  }
}

/**
 * æ ¹æ®æ¨¡å‹é…ç½®åŠ¨æ€åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ª LangChain ChatModel å®ä¾‹
 * è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒæŠ½è±¡ - æ‰€æœ‰æ¨¡å‹éƒ½é€šè¿‡è¿™ä¸ªå·¥å‚ç»Ÿä¸€åˆ›å»º
 *
 * @param {object} modelPayload - æ¥è‡ªå‰ç«¯æˆ–APIçš„å®Œæ•´æ¨¡å‹é…ç½®å¯¹è±¡
 * @param {string} modelPayload.id - æ¨¡å‹ID
 * @param {string} modelPayload.provider - æ¨¡å‹æä¾›å•†
 * @param {string} [modelPayload.apiKey] - APIå¯†é’¥ (å¯¹äºéœ€è¦å¯†é’¥çš„æ¨¡å‹)
 * @param {object} [modelPayload.config] - è‡ªå®šä¹‰æ¨¡å‹é…ç½® (ç”¨äºCustom provider)
 * @returns {import("@langchain/core/language_models/chat_models").BaseChatModel}
 * @throws {Error} å½“æ¨¡å‹é…ç½®æ— æ•ˆæˆ–åˆ›å»ºå¤±è´¥æ—¶
 */
export function createChatModel(modelPayload) {
  if (!modelPayload) {
    throw new Error("æ¨¡å‹é…ç½®(modelPayload)ä¸èƒ½ä¸ºç©º");
  }

  const { id, provider, config, apiKey } = modelPayload;

  console.log(`ğŸ­ LLMå·¥å‚å‡†å¤‡åˆ›å»ºæ¨¡å‹: ${id} (${provider})`);

  try {
    // 1. å¤„ç†è‡ªå®šä¹‰æ¨¡å‹ (Provider: 'Custom')
    if (provider === "Custom") {
      if (!config || !config.baseUrl || !config.apiKey || !config.model) {
        throw new Error("è‡ªå®šä¹‰æ¨¡å‹é…ç½®ä¸å®Œæ•´ï¼šéœ€è¦baseUrlã€apiKeyå’Œmodelå­—æ®µ");
      }
      console.log(`ğŸ”— ä½¿ç”¨è‡ªå®šä¹‰OpenAIå…¼å®¹æ¥å£: ${config.baseUrl}`);
      return new ChatOpenAI({
        apiKey: config.apiKey,
        modelName: config.model,
        configuration: { baseURL: config.baseUrl },
        temperature: 0.7,
        maxTokens: 2000,
      });
    }

    // 2. å¤„ç†é¢„å®šä¹‰æ¨¡å‹ (ä» config/models.js ä¸­è·å–é…ç½®)
    const modelConfig = getModelById(id);
    if (!modelConfig) {
      throw new Error(`æœªåœ¨ config/models.js ä¸­æ‰¾åˆ°æ¨¡å‹é…ç½®: ${id}`);
    }

    const { modelId, is_openai_compatible, base_url } = modelConfig;

    // 2a. å¤„ç† OpenAI å…¼å®¹çš„é¢„å®šä¹‰æ¨¡å‹
    if (is_openai_compatible) {
      console.log(`ğŸ”— ä½¿ç”¨é¢„å®šä¹‰OpenAIå…¼å®¹æ¥å£: ${base_url}`);
      return new ChatOpenAI({
        apiKey: apiKey, // ç›´æ¥ä» modelPayload è·å– apiKey
        modelName: modelId,
        configuration: { baseURL: base_url },
        temperature: 0.7,
        maxTokens: 2000,
      });
    }

    // 2b. å¤„ç†éå…¼å®¹çš„ç‰¹æ®Šé¢„å®šä¹‰æ¨¡å‹
    switch (provider) {
      case "Ollama": {
        const ollamaConfig = getConfig("ai.ollama");
        const baseUrl = ollamaConfig?.baseUrl || "http://localhost:11434";
        console.log(`ğŸ–¥ï¸ ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹: ${baseUrl}`);
        return new ChatOllama({
          baseUrl: baseUrl,
          model: modelId,
          temperature: 0.7,
        });
      }

      case "ModelScope": {
        console.log(`âš¡ ä½¿ç”¨è‡ªå®šä¹‰DashScopeæ¨¡å‹: ${modelId}`);
        return new CustomDashScopeChat({
          apiKey: apiKey, // ç›´æ¥ä» modelPayload è·å– apiKey
          modelId: modelId,
          baseUrl: base_url,
          temperature: 0.7,
        });
      }

      default:
        throw new Error(
          `ä¸æ”¯æŒçš„ LLM provider: ${provider}ã€‚è¯·æ£€æŸ¥æ¨¡å‹é…ç½®æˆ–å®ç°å¯¹åº”çš„æ¨¡å‹ç±»ã€‚`
        );
    }
  } catch (error) {
    console.error(`âŒ åˆ›å»ºæ¨¡å‹ '${id}' å¤±è´¥:`, error.message);
    // æŠ›å‡ºæ›´å…·ä½“çš„é”™è¯¯ï¼Œä¾¿äºè°ƒè¯•
    throw new Error(`æ— æ³•åˆ›å»ºæ¨¡å‹ '${id}' (${provider}): ${error.message}`);
  }
}

// --- è¾…åŠ©å‡½æ•° ---

/**
 * è·å–æ”¯æŒçš„æ¨¡å‹æä¾›å•†åˆ—è¡¨
 * @returns {string[]} æä¾›å•†åç§°æ•°ç»„
 */
export function getSupportedProviders() {
  return ["Ollama", "OpenAI", "æ™ºè°±", "ModelScope", "Custom"];
}

/**
 * æ£€æŸ¥æ˜¯å¦ä¸ºOpenAIå…¼å®¹æ¨¡å‹
 * @param {object} modelConfig - æ¨¡å‹é…ç½®
 * @returns {boolean} æ˜¯å¦ä¸ºOpenAIå…¼å®¹
 */
export function isOpenAICompatible(modelConfig) {
  return Boolean(modelConfig?.is_openai_compatible);
}
